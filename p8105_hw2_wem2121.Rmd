---
title: "Homework 2"
author: "Wayne Monical wem2121"
date: "2024-10-01"
output: github_document
---


```{r, results = 'hide'}
library(tidyverse)
library(readxl)
```




## Problem 1

### Data Cleaning

We begin by importing and cleaning the subway data set. This data set contains information on the organization, location, and amenities in the New York city subway system. The data is tidy. The data is consistent, i.e. each row of the data frame contains information on one subway entrance or exit. It is structured, i.e. every variable, such as station, line, and latitude, has its own column. Finally, all values have their own cell, i.e. there is no value concatenation. 

```{r}
subway = read_csv(
  "data/NYC_Transit_Subway_Entrance_And_Exit_Data.csv") |> 
  janitor::clean_names()
```


```{r}
subway_cols = c("line", "station_name", "station_latitude", "station_longitude",
                   "route1", "route2", "route3", "route4", "route5", "route6", "route7", 
                   "route8", "route9", "route10", "route11", "entrance_type", "entry",
                   "exit_only", "vending", "ada",    "ada_notes", "entrance_latitude",
                   "entrance_longitude")

subway = subway |> select(all_of(subway_cols))
```

We see here that `entry`, `exit_only`, and `vending` are character vectors, and `ada` is a logical vector
```{r}
logical_vars = c('entry', 'exit_only', 'vending', 'ada')

subway |> 
  select(all_of(logical_vars)) |> 
  summary()
```

Inspecting the unique values of each character, we can find the character strings that correspond to each logical value.
```{r}
char_vars = c('entry', 'exit_only', 'vending')

for(var in char_vars){
  print(var)
  
  subway |> 
    pull(var) |> 
    unique() |> 
    print()
}
```

Using the `mutate()` function, we can reassign these variables to be logical based on the character strings that we found.
```{r}
subway = subway |> 
  mutate(
    entry = entry == 'YES',
    exit_only = !is.na(exit_only),
    vending = vending == 'YES')
```


There are 465 distinct subway stations.
```{r}
subway |> select('station_name', 'line') |> distinct() |> nrow()
```

We must also standardize the route variables as characters. 
```{r}
subway = subway |> 
  mutate(
    route1 = as.character(route1),
    route2 = as.character(route2),
    route3 = as.character(route3),
    route4 = as.character(route4),
    route5 = as.character(route5),
    route6 = as.character(route6),
    route7 = as.character(route7),
    route8 = as.character(route8),
    route9 = as.character(route9),
    route10 = as.character(route10),
    route11 = as.character(route11),
  )
```



### Q & A

There are 84 ADA compliant subway stations. 
```{r}
subway |> filter(ada) |> select('station_name', 'line') |> distinct() |> nrow()
```

73% of station entrances and exits without vending allow entrance. 
```{r}
subway |> filter(!vending) |> pull(exit_only) |> summary()

133 / (50 + 133)
```

### Pivoting 
 
 Here, we pivot the route variables to a longer format and drop the rows that do not have a route. 
```{r}
subway= subway |> 
  pivot_longer(
    route1:route11,
    names_to = 'route_number',
    values_to = 'route'
  ) |> 
  filter(!is.na(route))
```

Using the code below, we find that 17 of the 43 of the stations on the A, 39%, are ADA compliant.  
```{r}
subway |> 
  filter(route == 'A') |> 
  select('line', 'station_name', 'ada') |>
  distinct() |> 
  pull(ada) |>
  summary()

107 / (166 + 107)
```

## Problem 2

Here we reading in the 2024 Mr Trash Wheel sheet using the `read_excel` function. We have specified the excel sheet and the data range, and we have rounded the number of sports balls and set it to integer values. We also transform the year variable into an integer, so that when we combine this data set with Gwynnda Trash Wheel's data set, we are able to smoothly bind the rows. We have omitted the totalling rows from the original excel sheet, since they do not contain dumpter-specific data. 
```{r}
mr_trash = 
  read_excel(
    path = 'data/202409 Trash Wheel Collection Data.xlsx',
    sheet = 'Mr. Trash Wheel',
    range = 'A2:N653') |> 
  janitor::clean_names() |> 
  mutate(
    sports_balls = sports_balls |> round() |> as.integer(),
    year = year |> as.integer())
```




Using the same code structure, we can import and clean Gwynnda Trash Wheel's data to create a single tidy data set. This data set is tidy because each row corresponds to the data from a single dumpster, each column corresponds to a single variable, and each cell contains a single value. 
```{r}
gwyndda_trash = 
  read_excel(
    path = 'data/202409 Trash Wheel Collection Data.xlsx',
    sheet = 'Gwynnda Trash Wheel',
    range = 'A2:K265') |> 
  janitor::clean_names()
```

Adding a variable for `trash_wheel`, we can combine Mr Trash Wheel's data with Gwynnda's data with the `bind_rows` function, stacking the rows on top of each other. Note that there are some differences between the two data sets. For example, Gwynnda Trash Wheel has no `sports_balls` variable, so this variable is `NA` in Gwynnda's rows. 

```{r}
mr_trash = mr_trash |> mutate(trash_wheel = 'mr_trash')

gwyndda_trash = gwyndda_trash |> mutate(trash_wheel = 'gwynnda')

trash = bind_rows(mr_trash, gwyndda_trash)
```


With this tidy data set, we can use `dyplr` functions to answer any concrete question about the data. For example, from `r trash |> filter(trash_wheel == 'mr_trash') |> pull(year) |> min()` to `r trash |> filter(trash_wheel == 'mr_trash') |> pull(year) |> max()`, Mr. Trash Wheel collected a total of `r trash |> filter(trash_wheel == 'mr_trash') |> pull(weight_tons) |> sum() |> round()` tons of trash.

```{r}
trash |> filter(trash_wheel == 'mr_trash') |> pull(weight_tons) |> sum()
```

By filtering for bins collected by Gwynnda in June of 2022, we can find that she collected a total of `r trash |> 
  filter(
    trash_wheel == 'gwynnda',
    year == 2022,
    month == 'June') |> 
  pull(cigarette_butts) |> 
  sum()` cigarette butts in that month. 
```{r}
trash |> 
  filter(
    trash_wheel == 'gwynnda',
    year == 2022,
    month == 'June') |> 
  pull(cigarette_butts) |> sum()
```



## Problem 3

Import and combine bakers.csv, bakes.csv, and results.csv to make a tidy dataset


The first step is to import the data and look at it. Any non-tidy data can be corrected at this stage. 

1) The `bakers` and `bakes` data is structured as expected, but the `results` data is not. Examining the csv file, we can see that the data frame begins on the third row. Specifying the argument `skip = 2` instructs the `read_csv()` function to skip the first two rows of the csv file, and therefore read the data as we expect. 

2) The data will be joined by the first names of the bakers, their series, and their episode. Therefore we will create the variable for first name in the `bakers` data frame by extracting the first word from the `baker_name` variable.

3) The `bakes` data frame has the column `signature_bakes` with the value "N/A", which should be treated as `NA`. We can specify this as a missing value in the `reaed_csv` function.
```{r}
bakers =
  read_csv('data/gbb_datasets/bakers.csv') |> 
  janitor::clean_names() |> 
  mutate(baker = stringr::word(baker_name, 1))

bakes =
  read_csv(
    'data/gbb_datasets/bakes.csv',
    na = c('N/A')) |> 
  janitor::clean_names()

results =
  read_csv('data/gbb_datasets/results.csv', skip = 2) |> 
  janitor::clean_names()
```


The second step is to check for completeness. 


Are all bakers featured in at least one episode?
```{r}
nrow(anti_join(bakers, results))
anti_join(bakers, bakes)
```

```{r}
bakes |> filter(!baker %in%  bakers$baker ) 
```

```{r}
results |> filter(!baker %in%  bakers$baker ) 
```



```{r}
results$baker |> head()
```


```{r}
bakes$baker |> head()
```



```{r}
bakers_missing_from_bakes = bakers |> filter(baker)
```

```{r}
nrow(anti_join(bakes, results))
nrow(anti_join(results, bakes))
```




```{r}
bakers |> select(baker_name, baker_occupation) |> anti_join(bakes) 
```


```{r}
nrow(anti_join(results, bakers))
```
```{r}
nrow(anti_join(bakers, results))
```

```{r}
nrow(anti_join(bakes, bakers))
```

```{r}
nrow(anti_join(bakes, results))
```

```{r}
nrow(anti_join(bakers, bakes))
```

```{r}
bakes |> filter(baker == 'Alice Fevronia')
```


```{r}
head(bakers)
```

```{r}
head(bakes)
```

```{r}
head(results)
```


```{r}
viewers =
  read_csv('data/gbb_datasets/viewers.csv') |> 
  janitor::clean_names()
```


check for completeness and correctness by using antijoin?

export to csv






